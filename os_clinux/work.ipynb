{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2VUCcpqmv7Y"
   },
   "source": [
    "# DataFrame 마무리 & Covariance Matrix(1)\n",
    "- - -\n",
    "\n",
    "#### 1. Performing a `Map` command in DataFrame\n",
    "* In order to perform a map on a dataframe, you first need to transform it into an RDD!\n",
    "* Not the recommended way. Better to use built-in sparkSQL functions\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Covariance Matrix\n",
    "\n",
    "* Calculating the mean of Sample Vectors\n",
    "\n",
    "* Outer product of sample vectors\n",
    "\n",
    "* Covariance Matrix\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zi0a3IDGWkc_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYVwZ8pCmv7a"
   },
   "source": [
    "### 1. Performing a `Map` command in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMUkFBYmmv7b"
   },
   "source": [
    "#### pyspark import & SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rey9WZEmv7b",
    "outputId": "d4d5f2c3-872c-4efe-f617-128c557c75f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.context.SQLContext object at 0x000001636DC895C8>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "sc = SparkContext(master=\"local[*]\")\n",
    "print(sc)\n",
    "\n",
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sI4sev3LWnRX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GoogleDriveDownloader in c:\\users\\win10-a287\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install GoogleDriveDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aI2e3ctxmv7e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'rm'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "x NY.parquet/\n",
      "x NY.parquet/_SUCCESS\n",
      "x NY.parquet/part-00022-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00000-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00021-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00001-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00023-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00002-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00024-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00003-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00025-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00004-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00027-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00005-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00006-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00007-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00008-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00009-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00010-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00011-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00012-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00013-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00014-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00015-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00016-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00017-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00018-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00019-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00020-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "x NY.parquet/part-00026-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#### 예제 파일 다운로드\n",
    "from os.path import exists\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import tarfile\n",
    "\n",
    "if exists(\"./NY.tgz\"):\n",
    "    !rm -rf ./NY.tgz\n",
    "if exists(\"./NY.parquet\"):\n",
    "    !rm -rf ./NY.parquet\n",
    "    \n",
    "gdd.download_file_from_google_drive(file_id='1hAHV6vC6FvVgrYnoN-lR-IfH488-H121',\n",
    "                                   dest_path = './NY.tgz')\n",
    "!tar -xzvf NY.tgz\n",
    "df = sqlContext.read.load(\"NY.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUNu1g5Tmv7f",
    "outputId": "0cb953c2-9f95-4951-f4f6-e00b731f0013",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----+--------------------+-----------------+-----------------+------------------+------------------+-----+-----------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|         latitude|         longitude|         elevation|state|       name|\n",
      "+-----------+-----------+----+--------------------+-----------------+-----------------+------------------+------------------+-----+-----------+\n",
      "|USW00014743|   TMIN_s20|1923|[2B D8 3D D8 4D D...|388.9079895019531|44.57720184326172|-75.10970306396484|136.60000610351562|   NY|CANTON 4 SE|\n",
      "|USW00014743|   TMIN_s20|1926|[2D D7 46 D7 5D D...|388.9079895019531|44.57720184326172|-75.10970306396484|136.60000610351562|   NY|CANTON 4 SE|\n",
      "|USW00014743|   TMIN_s20|1942|[6C D7 81 D7 95 D...|388.9079895019531|44.57720184326172|-75.10970306396484|136.60000610351562|   NY|CANTON 4 SE|\n",
      "+-----------+-----------+----+--------------------+-----------------+-----------------+------------------+------------------+-----+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema()와 sample를 이용한 데이터 확인\n",
    "df.printSchema()\n",
    "# sample 사용법 참조\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample\n",
    "df.sample(False, 0.01).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kodkuAVSmv7h"
   },
   "source": [
    "* DataFrame to RDD : `[DataFrame].rdd`, 각각의 요소가 `Row`인 RDD 생성\n",
    "\n",
    "* RDD to DataFrame : `sqlContext.createDataFrame([RDD], schema)`\n",
    "\n",
    "단, RDD에서 DataFrame으로 변환시, schema를 꼭 정의해줘야 한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GAcp0FCmv7h",
    "outputId": "c35586f2-9ba1-4f7e-808c-02c86a15a7cb",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Station='USC00309072', Measurement='SNWD_s20', Year=2005, Values=bytearray(b\"\\x0cT/TQTsT\\x95T\\xb7T\\xd8T\\xf8T\\x17U7UVUtU\\x90U\\xacU\\xc6U\\xdeU\\xf4U\\tV\\x1dV/V?VNVZVeVoVvV|V\\x80V\\x83V\\x83V\\x83V\\x80V~VwVqVhV_VUVKV?V2V$V\\x17V\\tV\\xf9U\\xe9U\\xd7U\\xc5U\\xb3U\\xa1U\\x8eUzUfUQU;U%U\\x0cU\\xf4T\\xdbT\\xc2T\\xa8T\\x8eTsTYT?T%T\\nT\\xdcS\\xa4SkS/S\\xf4R\\xbcR~R@R\\x03R\\xc6Q\\x8bQPQ\\x16Q\\xdeP\\xa9PuPBP\\x10P\\xbcO[O\\xfeN\\xa4NMN\\xfbM\\xabM]M\\x12M\\xc9L\\x83LAL\\x01L\\x89K\\x12K\\x9cJ+J\\xbcIUI\\xf1H\\x94H9H\\xcdG2G\\xa1F\\x0eF\\x81E\\x07E\\x96D,D\\x9dC\\xf8BrB\\xf3A|A\\x0bA\\xa1@=@\\xbe?\\x0e?~>\\x0b>\\x9d=4=\\xd1<s<\\x1a<\\x8c;I:\\xca9S9\\xe28y8\\x178v7\\xcc6.6\\x9b5\\x1e4\\xe90\\x890.0\\xaf/\\x0b/q.\\xe1-Y-\\xdb,f,\\xf1+(+o*\\xc4)))\\x9a(\\x19(F\\'q&\\xb0%\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xad%n&B\\'\\x16(\\x97(%)L,\\x910L3\\x8e5?6\\x027\\xd97a8>92:@;\\x1d<\\xa5<#=\\xc2=\\x9d>\\xa3?a@\\xfe@\\xa9AbB,C\\x14D\\x98D\\'E\\xc1EhF\\x1cG\\xddGTH\\xbdH*I\\x9bI\\x15J\\x97J\\x1dK\\xa6K\\x1bLfL\\xb3L\\x03MUM\\xaaM\\x03N^N\\xbcN\\x1dO\\x7fO\\xe2O#PUP\\x86P\\xb9P\\xeaP\\x1bQLQ{Q\\xaaQ\\xd7Q\\x02R,RTR{R\\xa0R\\xc1R\\xe2R\\xfdR\\x16S,S?SNSZSbSgSjSjSgS`SVSIS8S$S\\rS\\xf3R\\xd7R\\xb8R\\x97RtR\"), dist_coast=326.3900146484375, latitude=42.1171989440918, longitude=-77.94750213623047, elevation=460.20001220703125, state='NY', name='WELLSVILLE')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_rdd = df.rdd.takeSample(False, 1)\n",
    "some_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RC9wok3kmv7j"
   },
   "source": [
    "* Lec 6의 예제\n",
    "\n",
    "    1. 주어진 DataFrame에서 `Year가 1900 미만인 경우 '19th'`, `2000 미만인 경우 '20th'`, `2010 미만인 경우 '21st'`, `모두 아닐 경우 'possibly_bad_data'로 값을 치환`하여라\n",
    "\n",
    " 여기서 `map`에 들어가는 `input`이 무엇인지 반드시 숙지하여야 한다.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSSFoeotmv7k",
    "outputId": "f2713cb4-283f-47fd-a597-5c859bfb5a74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20th', '20th', '20th', '20th', '20th']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_century(row):\n",
    "    if row.Year < 1900:\n",
    "        return \"19th\"\n",
    "    elif row.Year < 2000:\n",
    "        return \"20th\"\n",
    "    elif row.Year < 2010:\n",
    "        return \"21st\"\n",
    "    else:\n",
    "        return \"possibly_bad_data\"\n",
    "    \n",
    "df.rdd.map(find_century).take(5) # map에 들어가는 input값은 rdd list가 되는게 맞는지 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzFXzrhmmv7l"
   },
   "source": [
    "* Lec 6의 예제\n",
    "\n",
    "    2. 주어진 DataFrame에서 각각의 요소 중 `longitude`와 `latitude`를 추출하여 (longitude, latitude)의 형태로 값을 출력하여라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2KNPUc0mv7m",
    "outputId": "7aa18389-8ca7-4356-bcca-9848b2433dee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda row : (row.longitude, row.latitude)).take(5) # 데이터 프레임의 필드의 종류인 위도와 경도를 rdd 코드로 row 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAz482zDmv7n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OuM3jhrmv7p"
   },
   "source": [
    "### Excercise 1 - RDD function in DataFrame (60 point)\n",
    "\n",
    "- - -\n",
    "task 별로 ``위에서 사용한 DataFrame(df)``에서 ``RDD``로 변환 후 ``RDD function``을 적용하여 해결합니다\n",
    "\n",
    "(task 당 20 point)\n",
    "\n",
    "---\n",
    "\n",
    "**task**\n",
    "\n",
    "* 1 : ``df``에서 ``name``별 가장 최근 ``Year``를 **map과 reduce, 또는 reduceByKey 등**을 활용하여 구한 후, take(10)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 2 : ``df``에서 ``Year``가 ``2000 이상``인 결과에 대해, ``name``별 ``Year``, ``dist_coast``, ``elevation``의 평균을 구하고 ``name``를 기준으로 정렬(Z->A)한 후, take(5)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 3 : ``df``에서 ``Measurement`` 별 `Values`의 1일부터 10일까지(``np.frombuffer(row.Values[:20], dtype='float16')`` 또는 ``np.frombuffer(row.Values, dtype = 'float16')[:10])의 합이 가장 큰 ``Year``와 ``그 값을 구한 후``, ``Measurement``를 기준으로 정렬(A->Z)합니다. 마지막으로 collect를 하여 출력합니다.(20 point) \n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNu7gzmpmv7p"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 1 : ``df``에서 ``name``별 가장 최근 ``Year``를 **map과 reduce, 또는 reduceByKey 등**을 활용하여 구한 후, take(10)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "**★ DataFrame이 아닌 RDD로 작업할 것**\n",
    "\n",
    "```\n",
    "#task1 output\n",
    "[('DANSVILLE MUNI AP', 2013),\n",
    " ('BRIDGEHAMPTON', 2013),\n",
    " ('MIDDLETOWN 2 NW', 2011),\n",
    " ('BERLIN 5 S', 2000),\n",
    " ('ELMIRA CORNING RGNL AP', 2013),\n",
    " ('UNADILLA 2 N', 2013),\n",
    " ('SUFFERN 2 E', 1955),\n",
    " ('ROXBURY', 1972),\n",
    " ('LOWVILLE', 2013),\n",
    " ('GABRIELS', 1978)]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5Qtu4q6mv7p",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DANSVILLE MUNI AP', 2013),\n",
       " ('BRIDGEHAMPTON', 2013),\n",
       " ('MIDDLETOWN 2 NW', 2011),\n",
       " ('BERLIN 5 S', 2000),\n",
       " ('ELMIRA CORNING RGNL AP', 2013),\n",
       " ('UNADILLA 2 N', 2013),\n",
       " ('SUFFERN 2 E', 1955),\n",
       " ('ROXBURY', 1972),\n",
       " ('LOWVILLE', 2013),\n",
       " ('GABRIELS', 1978)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-1 답 작성\n",
    "# map을 이용하여 (name, Year) 형태로 mapping 하고\n",
    "# reduceByKey에 가장 최근, 즉 가장 큰값을 찾기위해 \n",
    "# max 함수를 인자로 넘겨주어 최대값을 찾음\n",
    "# 마지막으로 take(10)으로 10개를 보여줌\n",
    "df.rdd.map(lambda row : (row.name, row.Year)).reduceByKey(max).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2xTPOidmv7r"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 2 : ``df``에서 ``Year``가 ``2000 이상``인 결과에 대해, ``name``별 ``Year``, ``dist_coast``, ``elevation``의 평균을 구하고 ``name``를 기준으로 정렬(Z->A)한 후, take(10)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "**★ DataFrame이 아닌 RDD로 작업할 것**\n",
    "\n",
    "```\n",
    "# task2 output\n",
    "[('YOUNGSTOWN 2 NE', (2008.5, 476.80999755859375, 85.30000305175781)),\n",
    " ('YORKTOWN HEIGHTS 1W',\n",
    "  (2006.421686746988, 28.945999145507812, 204.1999969482422)),\n",
    " ('WINDHAM 3 E', (2004.8387096774193, 147.28700256347656, 512.0999755859375)),\n",
    " ('WILLSBORO 1 N',\n",
    "  (2005.360655737705, 255.24200439453125, 54.900001525878906)),\n",
    " ('WHITNEY POINT DAM', (2009.2826086956522, 235.35800170898438, 317.0))]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-nU-RqQmv7r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YOUNGSTOWN 2 NE', (2008.5, 476.80999755859375, 85.30000305175781)),\n",
       " ('YORKTOWN HEIGHTS 1W',\n",
       "  (2006.421686746988, 28.945999145507812, 204.1999969482422)),\n",
       " ('WINDHAM 3 E', (2004.8387096774193, 147.28700256347656, 512.0999755859375)),\n",
       " ('WILLSBORO 1 N',\n",
       "  (2005.360655737705, 255.24200439453125, 54.900001525878906)),\n",
       " ('WHITNEY POINT DAM', (2009.2826086956522, 235.35800170898438, 317.0)),\n",
       " ('WHITEHALL', (2006.421686746988, 220.0189971923828, 36.29999923706055)),\n",
       " ('WESTHAMPTN GABRESKI AP',\n",
       "  (2006.1333333333334, 4.3188700675964355, 20.399999618530273)),\n",
       " ('WESTFIELD 2 SSE', (2001.5, 418.3210144042969, 215.5)),\n",
       " ('WESTCHESTER CO AP', (2005.734693877551, 7.586299896240234, 115.5)),\n",
       " ('WEST POINT', (2006.0, 47.6151008605957, 97.5))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-2 답 작성\n",
    "# name을 key로 하여 Year, dist_coast, elevation을 value로 가지는 rdd를 3개 mapping함\n",
    "# map에는 2000>=Year 경우만 처리하는 함수를 인자로 넘겨줌\n",
    "# filter를 통해 None을 제거해줌\n",
    "# 그리고 groupByKey를 통해 같은 key끼리 grouping 해줌\n",
    "# 만들어진 각각 rdd를 mapValues와 적절한 lambda로 평균을 구함\n",
    "# 그후에 join을 통하여 합침\n",
    "# 합치고 map을 통해 적절한 형태로 mapping 해줌\n",
    "def func1(row):\n",
    "    if row.Year>=2000:\n",
    "        return row.name, row.Year\n",
    "def func2(row):\n",
    "    if row.Year>=2000:\n",
    "        return row.name, row.dist_coast\n",
    "def func3(row):\n",
    "    if row.Year>=2000:\n",
    "        return row.name, row.elevation\n",
    "temp1 = df.rdd.map(func1).filter(lambda x: x is not None).groupByKey().mapValues(lambda row : sum(row)/len(row))\n",
    "temp2 = df.rdd.map(func2).filter(lambda x: x is not None).groupByKey().mapValues(lambda row : sum(row)/len(row))\n",
    "temp3 = df.rdd.map(func3).filter(lambda x: x is not None).groupByKey().mapValues(lambda row : sum(row)/len(row))\n",
    "ret = temp1.join(temp2).join(temp3).map(lambda x: (x[0], (x[1][0][0], x[1][0][1], x[1][1]) ) ).sortByKey(ascending=False)\n",
    "ret.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjP5V4xymv7w"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 3 : ``df``에서 ``Measurement`` 별 `Values`의 1일부터 10일까지(``np.frombuffer(row.Values[:20], dtype='float16')`` 또는 ``np.frombuffer(row.Values, dtype = 'float16')[:10])의 합이 가장 큰 ``Year``와 ``그 값을 구한 후``, ``Measurement``를 기준으로 정렬(A->Z)합니다. 마지막으로 collect를 하여 출력합니다.(20 point) \n",
    "\n",
    "<br>\n",
    "\n",
    "**★ DataFrame이 아닌 RDD로 작업할 것**\n",
    "\n",
    "★★여기서 ``Values``는 ``bytearray`` type입니다. ★★\n",
    "\n",
    "``numpy``의 **frombuffer**를 이용하여 ``float16``으로 바꿉니다.\n",
    "자세한 사용법은 [여기](https://docs.scipy.org/doc/numpy/reference/generated/numpy.frombuffer.html)를 참고합니다\n",
    "\n",
    "★★또한, ``numpy``의 ``nansum``을 이용하여 값이 **nan**이 아닌 Values의 합을 구합니다.★★\n",
    "\n",
    "```\n",
    "# task3 output\n",
    "[('PRCP', (12824.0, 1946)),\n",
    " ('PRCP_s20', (4264.0, 1983)),\n",
    " ('SNOW', (2912.0, 1954)),\n",
    " ('SNOW_s20', (4140.0, 1895)),\n",
    " ('SNWD', (16590.0, 1970)),\n",
    " ('SNWD_s20', (12320.0, 1976)),\n",
    " ('TMAX', (2216.0, 1897)),\n",
    " ('TMAX_s20', (2050.0, 1897)),\n",
    " ('TMIN', (623.0, 2007)),\n",
    " ('TMIN_s20', (21360.0, 1987)),\n",
    " ('TOBS', (1000.0, 1998)),\n",
    " ('TOBS_s20', (969.5, 1992))]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wBSUHVbmv7w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRCP', (12824.0, 1946)),\n",
       " ('PRCP_s20', (4264.0, 1964)),\n",
       " ('SNOW', (2912.0, 1904)),\n",
       " ('SNOW_s20', (4140.0, 1895)),\n",
       " ('SNWD', (16590.0, 1970)),\n",
       " ('SNWD_s20', (12320.0, 1976)),\n",
       " ('TMAX', (2216.0, 1897)),\n",
       " ('TMAX_s20', (2050.0, 1897)),\n",
       " ('TMIN', (623.0, 1898)),\n",
       " ('TMIN_s20', (21360.0, 1987)),\n",
       " ('TOBS', (1000.0, 1902)),\n",
       " ('TOBS_s20', (969.5, 1992))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1-3 답 작성\n",
    "#np.frombuffer(row.Values[:20], dtype='float16') # 또는 \n",
    "#np.frombuffer(row.Values, dtype = 'float16')[:10]\n",
    "# 먼저 2개의 (Measurement, sum of Values), (sum of Values, Year) 형태의 2개의 rdd를 만듬\n",
    "# 그리고 전자에 reduceByKey에 max를 인자로 넘겨주어 sum of Values의 최대값으로 reduce함\n",
    "# 그리고 map을 통해 적절한 형태로 mapping 해줌\n",
    "# join을 통해 합침\n",
    "# Measurement와 sum of values은 동일하나 Year 다른 경우가 존재함\n",
    "# 때문에 다시한번 reduceByKey에 min을 인자로 넘겨주어 가장 작은 Year를 구함\n",
    "# 다시한번 map을 통해 적절한 형태로 mapping 해주고\n",
    "# sortByKey로 정렬해주고\n",
    "# collect 해줌\n",
    "temp1 = df.rdd.map(lambda row: (row.Measurement, np.nansum(np.frombuffer(row.Values, dtype = 'float16')[:10]) ))\n",
    "temp2 = df.rdd.map(lambda row: (np.nansum(np.frombuffer(row.Values, dtype = 'float16')[:10]), row.Year))\n",
    "temp1 = temp1.reduceByKey(max).map(lambda x: (x[1], x[0]))\n",
    "ret = temp1.join(temp2).map(lambda x: ((x[1][0], x[0]), x[1][1])).reduceByKey(min).map(lambda x: (x[0][0], (x[0][1], x[1]))).sortByKey().collect()\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0Z5dZeD2ltF"
   },
   "source": [
    "### 2. Spark와 Numpy를 사용하여 Covariance Matrix 구해보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yt_7LRtgBY_a"
   },
   "source": [
    "#### (1) numpy 기초 \n",
    "아래 참고자료를 활용하여 numpy 기초 학습\n",
    "\n",
    "* 참고자료 \n",
    "  * [참고자료 1](http://taewan.kim/post/numpy_cheat_sheet/)\n",
    "  * [참고자료 2](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n",
    "  * [참고자료 3](https://scipy-lectures.org/intro/numpy/array_object.html#what-are-numpy-and-numpy-arrays)\n",
    "  * [참고자료 4](https://doorbw.tistory.com/171)\n",
    "  * [참고자료 5](https://datascienceschool.net/view-notebook/17608f897087478bbeac096438c716f6/)\n",
    "\n",
    "* 위 자료에서중에서도 \n",
    "  * ndarray 생성법\n",
    "  * vector, matrix 연산\n",
    "  * 인덱싱 (slicing)\n",
    "  * 행렬 합치기 (vstack, dstack, hstack)\n",
    "  * sum, mean\n",
    "  * np.nan 자료형\n",
    "  * reshape\n",
    "  * matmul\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9N7GWN5m0aj"
   },
   "source": [
    "#### (2)  Calculating the mean of Sample Vectors \n",
    "다음 벡터들을 샘플 벡터로 가정합니다.\n",
    "* $n$ 은 샘플의 수 이고\n",
    "* $d$는 각 데이터 벡터의 길이 입니다 (예를 들어서 날씨데이터의 경우 $d=365$)\n",
    "$$\n",
    "\\mathbf{x}_i = \\left[\\begin{array}{cccc}\n",
    "x_{i1} & x_{i2}& \\ldots & x_{id}, \n",
    "\\end{array}\\right], \\quad i=1,\\ldots, n\n",
    "$$\n",
    "Sample vector 들의 mean (평균) 벡터 $\\bar{\\mathbf{x}}$는 다음과 같이 구합니다 \n",
    "$$\n",
    "\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yl5QnvQZ4yvX"
   },
   "outputs": [],
   "source": [
    "# 평균 벡터 구하는 문제 (sum, mean 활용 둘다 해도 괜찮습니다))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELHX1CFB2ltL",
    "outputId": "4e7c71c5-3d6c-4c7e-99dd-c5837af7b920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 sample의 값 :  [1 2 3] [4 5 6] [7 8 9]\n",
      "sample vector sum :  [12 15 18]\n",
      "sample vector mean :  [4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "# in python\n",
    "import numpy as np\n",
    "\n",
    "sample1 = np.array([1,2,3])\n",
    "sample2 = np.array([4,5,6])\n",
    "sample3 = np.array([7,8,9])\n",
    "print(\"각 sample의 값 : \", sample1, sample2, sample3)\n",
    "print(\"sample vector sum : \", sample1 + sample2 + sample3)\n",
    "print(\"sample vector mean : \", (sample1 + sample2 + sample3) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMYlw8JX9f7u",
    "outputId": "0be9a8c8-9ba1-462b-fc10-ef3b58a89d60",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 sample의 값 :  [array([1, 2, 3]), array([4, 5, 6]), array([7, 8, 9])]\n",
      "sample vector sum :  [12 15 18]\n",
      "sample vector mean :  [4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "# in spark\n",
    "vector_list = sc.parallelize([np.array([1,2,3]),np.array([4,5,6]),np.array([7,8,9])])\n",
    "print(\"각 sample의 값 : \", vector_list.collect())\n",
    "print(\"sample vector sum : \", vector_list.reduce(lambda ndarr1, ndarr2 : ndarr1 + ndarr2))\n",
    "print(\"sample vector mean : \",\n",
    "      vector_list.reduce(lambda ndarr1, ndarr2 : ndarr1 + ndarr2) / vector_list.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vVD-9dp2ltR"
   },
   "source": [
    "### Excercise 2 - Calculating the mean of Sample Vectors (40 point)\n",
    "\n",
    "- - -\n",
    " \n",
    "다음 데이터에 대하여 다음 과제를 수행하세요.\n",
    "\n",
    "- regular.csv : KBO에서 활약한 타자들의 역대 정규시즌 성적을 포함하여 몸무게, 키 ,생년월일 등의 기본정보\n",
    "\n",
    "**위의 두 데이터는 모두 `,`로 구분되어 있습니다.**\n",
    "\n",
    " - **데이터의 자세한 설명은 다음의 링크를 참조해주세요.([여기를 눌러서 12. 데이터 설명 참고](https://dacon.io/cpt6/62885))**\n",
    " - 또한 regular.csv를 직접 열어서 데이터가 어떻게 저장되어 있는지 확인해주세요.\n",
    "\n",
    "- - -\n",
    "\n",
    "**task**\n",
    "\n",
    "- 1. filter를 사용하여 팀 이름이 ``두산``인 선수에 대해, ``(batter_id, np.array([G,R,H,RBI,BB]))``의 형태로 Key/Value RDD를 생성합니다. (20 point)\n",
    "\n",
    "    1. G R H RBI BB의 경우 초기 설정값이 ``stirng``.  이 값들을 ``float64``으로 변경할 것. ex) ``np.array([1,2,3], dtype = 'float64')``\n",
    "    2. ★ 각 값이 `' '`일 경우, 0 으로 변경할 것. \n",
    "    3. ``map``에서 바로 적용 또는 그러한 함수를 작성\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "- 2. (20 point)\n",
    "    1. ``reduceByKey``를 사용하여 ``batter_id``(Key)가 동일한 선수의 ``G, R, H, RBI, BB``(Value)를 각각 더해준 후 ``batter_id``(Key)를 기준으로 ``sortByKey``를 적용합니다. 그 후, ``map``을 사용하여 ``G, R, H, RBI, BB``(Value)만 선택 후 새로운 RDD로 만듭니다.\n",
    "    2. 위에서 생성된 RDD에 대해, 중복되지 않는 sample의 수를 ``count``를 이용하여 구합니다.\n",
    "    3. ``reduce``를 이용하여 ``G, R, H, RBI, BB``(Value)를 모두 더한 후 위에서 구한 sample의 수(`count`)로 나누어서 sample vector의 평균을 구합니다.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sE2zQXh92ltS"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1b_L-rJYJC9Oqga0fQ2zh2M763CTM8jzR\", \"regular.csv\")\n",
    "regular = sc.textFile(\"./regular.csv\").map(lambda x : x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsNIE2482ltW"
   },
   "source": [
    "**task**\n",
    "\n",
    "- 1. filter를 사용하여 팀 이름이 ``두산``인 선수에 대해, ``(batter_id, np.array([G,R,H,RBI,BB]))``의 형태로 Key/Value RDD를 생성합니다. (20 point)\n",
    "\n",
    "    1. G R H RBI BB의 경우 초기 설정값이 ``stirng``.  이 값들을 ``float64``으로 변경할 것. ex) ``np.array([1,2,3], dtype = 'float64')``\n",
    "    2. ★ 각 값이 `' '`일 경우, 0 으로 변경할 것. \n",
    "    3. ``map``에서 바로 적용 또는 그러한 함수를 작성\n",
    "    \n",
    "```\n",
    "#output\n",
    "[(7, array([1., 0., 0., 0., 0.])),\n",
    " (7, array([64., 16., 21., 11.,  6.])),\n",
    " (7, array([93., 30., 40., 25., 16.])),\n",
    " (7, array([6., 3., 3., 3., 2.])),\n",
    " (7, array([40., 14., 17.,  5., 10.])),\n",
    " (7, array([48., 12., 24., 10., 18.])),\n",
    " (17, array([16.,  1.,  1.,  1.,  0.])),\n",
    " (17, array([32.,  2.,  3.,  0.,  0.])),\n",
    " (17, array([16.,  2.,  2.,  0.,  0.])),\n",
    " (17, array([116.,  38.,  85.,  29.,  24.]))]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKmz4z682ltY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, array([1., 0., 0., 0., 0.])),\n",
       " (7, array([64., 16., 21., 11.,  6.])),\n",
       " (7, array([93., 30., 40., 25., 16.])),\n",
       " (7, array([6., 3., 3., 3., 2.])),\n",
       " (7, array([40., 14., 17.,  5., 10.])),\n",
       " (7, array([48., 12., 24., 10., 18.])),\n",
       " (17, array([16.,  1.,  1.,  1.,  0.])),\n",
       " (17, array([32.,  2.,  3.,  0.,  0.])),\n",
       " (17, array([16.,  2.,  2.,  0.,  0.])),\n",
       " (17, array([116.,  38.,  85.,  29.,  24.]))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stringToFloat(element): # G R H RBI BB\n",
    "    # 함수를 작성해서 사용해도 되고, 안 사용해도 됩니다.\n",
    "    return (int(element[0]), np.array(element[1:], dtype = 'float64'))\n",
    "\n",
    "# output\n",
    "# 먼저 데이터를 보면 row의 3번째에 이름이 온다는 것을 알 수 있음\n",
    "# 그리고 G R H RBI BB는 각각 5, 7, 8 ,13, 16번째에 온다는 것을 알 수 있음\n",
    "# 그러면 filter로 이름이 두산인 경우만 filtering 해주고\n",
    "# map을 통해 G R H RBI BB를 적절히 mapping 시켜줌\n",
    "# 또한 원래는 string 형태이지만 함수를 통해 G는 int로, 나머지는 float64로 변경시켜줌\n",
    "task1 = regular.filter(lambda row : row[3] == \"두산\").map(lambda row : stringToFloat([row[0], row[5], row[7], row[8], row[13], row[16]]))\n",
    "task1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVX2WOiY2lte"
   },
   "source": [
    "**task**\n",
    "\n",
    "- 2. (20 point)\n",
    "    1. ``reduceByKey``를 사용하여 ``batter_id``(Key)가 동일한 선수의 ``G, R, H, RBI, BB``(Value)를 각각 더해준 후 ``batter_id``(Key)를 기준으로 ``sortByKey``를 적용합니다. 그 후, ``map``을 사용하여 ``G, R, H, RBI, BB``(Value)만 선택 후 새로운 RDD로 만듭니다.\n",
    "    2. 위에서 생성된 RDD에 대해, 중복되지 않는 sample의 수를 ``count``를 이용하여 구합니다.\n",
    "    3. ``reduce``를 이용하여 ``G, R, H, RBI, BB``(Value)를 모두 더한 후 위에서 구한 sample의 수(`count`)로 나누어서 sample vector의 평균을 구합니다.\n",
    "\n",
    "```\n",
    "# output\n",
    "[G, R, H, RBI, BB] mean :  [427.8846  186.32692 344.05768 178.0577  129.48077]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVvi_9gy2ltg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[G, R, H, RBI, BB] mean :  [427.88461538 186.32692308 344.05769231 178.05769231 129.48076923]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey로 batter_id가 동일한 선수의 G R H RBI BB를 더해줌\n",
    "# sortByKey로 정렬\n",
    "# map을 통해 G R H RBI BB만 mapping 시켜주고 task2로 변수 지정\n",
    "# count를 통해 task2의 갯수를 구해주고 cnt로 변수 지정\n",
    "# reduce에 sum을 인자로 넘겨주어 총 합을 구하고 cnt로 나누어줌\n",
    "# 그리고 이 값을 task2_mean_vector로 지정함\n",
    "# output 예시에는 float32로 주어졌을때의 결과로 보여지지만\n",
    "# 위의 task1에서는 float64로 계산하라고 하였기 때문에 상이한 결과가 나타날 수 있음\n",
    "task2 = task1.reduceByKey(lambda x,y : np.asarray([x[i]+y[i] for i in range(len(x))])).sortByKey().map(lambda row : row[1:])\n",
    "cnt = task2.count()\n",
    "task2_mean_vector = (task2.reduce(sum)/cnt)[0]\n",
    "#output\n",
    "print(\"[G, R, H, RBI, BB] mean : \", task2_mean_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43q9xB_nvllT"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW7_upload_V4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
